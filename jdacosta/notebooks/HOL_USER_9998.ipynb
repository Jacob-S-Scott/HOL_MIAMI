{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "od2obobfn7o6zkn4aimg",
   "authorId": "7513526462763",
   "authorName": "HOL_USER_99",
   "authorEmail": "",
   "sessionId": "3778a8c1-b1b0-4d22-bcd2-ded9a6c1c27a",
   "lastEditTime": 1760624275172
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "d4fa4eb6-2042-4d43-9c61-fb18cc9aef48",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "import json\nimport pandas as pd\nimport os\nfrom snowflake.core import Root\nfrom snowflake.snowpark import Session",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a676932f-19ee-4012-a540-2543bbe99ee4",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "# HOL Setup Notebook\nRun the following commands to provision the necessary objects for the Snowflake Intelligence HOL\n\n- add snowflake package\n"
  },
  {
   "cell_type": "markdown",
   "id": "22d2233c-e24d-4234-9d49-3dd3a6e28f52",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## Setup User Schema"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "SCHEMA_SETUP",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "USE ROLE HOL_ADMIN;\n\nSET CURRENT_USER = CURRENT_USER();\n\nUSE DATABASE HOL;\nCREATE SCHEMA IF NOT EXISTS IDENTIFIER($CURRENT_USER);\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f43a6c3e-00cd-4d8d-b08f-468f49226c32",
   "metadata": {
    "language": "sql",
    "name": "WORK_IN_OWN_SCHEMA",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "USE SCHEMA IDENTIFIER($CURRENT_USER);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad8fa26e-6c47-4ae9-8b59-a8e3d54a4a61",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Setup Unstructured Chunks Table"
  },
  {
   "cell_type": "code",
   "id": "d659a194-5dbc-457e-b33e-ac1a96cbb0c0",
   "metadata": {
    "language": "sql",
    "name": "CREATE_TABLE_FOR_CHUNKS_OF_TEXT"
   },
   "outputs": [],
   "source": "\n-- CREATE OR REPLACE TABLE SEC_FILINGS_CHUNKS (\nCREATE TABLE IF NOT EXISTS SEC_FILINGS_CHUNKS (\n    document_name varchar,\n    chunk_id varchar,\n    chunk_text varchar,\n    document_date date\n);\n\n-- Perform OCR and Chunk Text\nINSERT INTO SEC_FILINGS_CHUNKS  \n    with doc_text as (\n        select\n            split_part(relative_path,'.',0) as document_name,\n            AI_PARSE_DOCUMENT (\n                TO_FILE('@DATA.SEC_FILINGS_STAGE',relative_path),\n                {'mode': 'OCR' , 'page_split': false}\n            ):content::varchar as doc_text,\n            to_date(split_part(document_name,'_',3),'MM-DD-YY') as doc_date\n        from directory('@DATA.SEC_FILINGS_STAGE')\n    )\n    \n    , chunked as (\n        select \n            document_name,\n            SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n              doc_text,\n              'none',\n              500\n            ) as chunks,\n            doc_date\n        from doc_text\n    )\n    \n    , flattened as (\n        select\n            document_name,\n            index as chunk_id,\n            value::varchar as chunk_text,\n            doc_date\n        from chunked,\n            TABLE(FLATTEN(INPUT => CHUNKS))\n    )\n    \n    SELECT * FROM flattened\n;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0aa1a8d7-2b95-4472-9a2e-3c79de4b3d15",
   "metadata": {
    "language": "sql",
    "name": "EXAMINE_CHUNK_DATA"
   },
   "outputs": [],
   "source": "SELECT * FROM SEC_FILINGS_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be197b9b-e87c-4b21-8b09-44b1fe527615",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Create Search Service for RAG"
  },
  {
   "cell_type": "code",
   "id": "9a28493b-d0b5-4d0b-853f-dcea35749a58",
   "metadata": {
    "language": "sql",
    "name": "CREATE_CORTEX_SEARCH_SERVICE"
   },
   "outputs": [],
   "source": "\nCREATE CORTEX SEARCH SERVICE IF NOT EXISTS SEC_FILINGS_SEARCH\n  ON CHUNK_TEXT\n  ATTRIBUTES DOCUMENT_DATE\n  WAREHOUSE = SNOWFLAKE_LEARNING_WH\n  TARGET_LAG = '7 DAYS'\n  AS (SELECT * FROM SEC_FILINGS_CHUNKS);\n\n-- RUN A SAMPLE QUERY\nSELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'SEC_FILINGS_SEARCH',\n    '{\n      \"query\": \"NVIDIA revenue guidance\",\n      \"columns\": [\"DOCUMENT_NAME\", \"CHUNK_ID\", \"DOCUMENT_DATE\", \"CHUNK_TEXT\"],\n      \"limit\": 5\n    }'\n  )\n)['results'] AS results;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17767969-a908-470d-a900-2c09da97d9de",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "## Create a Custom Tool for the Agent\nJust a dummy stock price tool, in reality this could easily be replaced with an API call using an external access integration."
  },
  {
   "cell_type": "code",
   "id": "7e410619-0d8c-41b7-87a9-897337893a08",
   "metadata": {
    "language": "sql",
    "name": "CREATE_CUSTOM_TOOL"
   },
   "outputs": [],
   "source": "-- Dummy Tool\nCREATE OR REPLACE PROCEDURE GET_STOCK_PRICE(TICKER STRING, EXCHANGE STRING)\nRETURNS FLOAT\nLANGUAGE SQL\nAS\n$$\nDECLARE\n    stock_price FLOAT;\nBEGIN\n    stock_price := 185.88;\n    RETURN stock_price;\nEND;\n$$;\n\ncall get_stock_price('nvda','nyse');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b83dc052-ce86-46a7-ae27-f7eae63b5362",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "## Verify Existing Price History Data\nFor structured data we will be using a preloaded table made available for the user."
  },
  {
   "cell_type": "code",
   "id": "df421e8e-a3ee-483b-a162-a4074366b313",
   "metadata": {
    "language": "sql",
    "name": "CREATE_OWN_PRICE_HISTORY_DATA"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE STOCK_PRICE_HISTORY CLONE DATA.STOCK_PRICE_HISTORY;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f792b0-e4a7-494b-b457-c6f8f0cce4d1",
   "metadata": {
    "language": "sql",
    "name": "EXAMINE_PRICE_HISTORY_DATA"
   },
   "outputs": [],
   "source": "SELECT * FROM STOCK_PRICE_HISTORY",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "112c411b-4ce4-44f4-aee1-1cfca51e7868",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "## Create the Semantic View\n\nThis could also be done a number of different ways, mainly through Snowsight. If you have YAML already provisioned in this case you can simply use the system function as shown below.\n\nNOTE: Make sure to change schema and database in the yaml below before creation to HOL.<YOUR_USER>"
  },
  {
   "cell_type": "code",
   "id": "5a8356ec-230d-4dbf-8540-68e21bcf246c",
   "metadata": {
    "language": "sql",
    "name": "CREATE_SEMANTIC_VIEW"
   },
   "outputs": [],
   "source": "CALL SYSTEM$CREATE_SEMANTIC_VIEW_FROM_YAML(\n  'HOL.' || $CURRENT_USER,\n  $$\nname: stock_prices_semantic_model\ndescription: Semantic model for historical daily stock prices (OHLCV) with dividends and splits.\ncustom_instructions: >\n  If the user doesn't specify a date filter, apply a default filter for the last year.\n  Round numeric outputs to 2 decimal places.\n  If the user asks for 'returns' or 'change', interpret as percentage change over\n  the selected period (e.g., (last_close - first_close) / first_close * 100).\n\ntables:\n  - name: stock_prices\n    description: Daily OHLCV per ticker.\n    base_table:\n      database: HOL\n      schema: jacob_scott\n      table: STOCK_PRICE_HISTORY\n\n    time_dimensions:\n      - name: date\n        synonyms: [\"trading date\", \"day\"]\n        description: Trading day (no time component).\n        expr: DATE\n        data_type: DATE\n        unique: false\n\n    dimensions:\n      - name: ticker\n        synonyms: [\"symbol\"]\n        description: Stock ticker symbol (e.g., NVDA).\n        expr: TICKER\n        data_type: TEXT\n        unique: false\n        is_enum: true\n\n    facts:\n      - name: open_price\n        synonyms: [\"open\"]\n        description: Opening price for the day.\n        expr: OPEN_PRICE\n        data_type: NUMBER\n\n      - name: high_price\n        synonyms: [\"high\"]\n        description: Intraday high price.\n        expr: HIGH_PRICE\n        data_type: NUMBER\n\n      - name: low_price\n        synonyms: [\"low\"]\n        description: Intraday low price.\n        expr: LOW_PRICE\n        data_type: NUMBER\n\n      - name: close_price\n        synonyms: [\"close\", \"closing price\"]\n        description: Closing price for the day.\n        expr: CLOSE_PRICE\n        data_type: NUMBER\n\n      - name: volume\n        synonyms: [\"shares traded\"]\n        description: Number of shares traded during the day.\n        expr: VOLUME\n        data_type: NUMBER\n\n      - name: dividends\n        synonyms: [\"dividend\"]\n        description: Cash dividends paid on the day.\n        expr: DIVIDENDS\n        data_type: NUMBER\n\n      - name: stock_splits\n        synonyms: [\"split ratio\"]\n        description: Stock split ratio for the day (e.g., 4.0 == 4-for-1).\n        expr: STOCK_SPLITS\n        data_type: NUMBER\n\n    metrics:\n      - name: avg_close_price\n        synonyms: [\"average close\"]\n        description: Average closing price over the selected period.\n        expr: AVG(CLOSE_PRICE)\n\n      - name: max_close_price\n        synonyms: [\"record close\", \"all-time high close\"]\n        description: Maximum closing price over the selected period.\n        expr: MAX(CLOSE_PRICE)\n\n      - name: min_close_price\n        synonyms: [\"lowest close\"]\n        description: Minimum closing price over the selected period.\n        expr: MIN(CLOSE_PRICE)\n\n      - name: total_volume\n        synonyms: [\"sum volume\", \"trading volume\"]\n        description: Total shares traded over the selected period.\n        expr: SUM(VOLUME)\n\n    filters:\n      - name: last_30_days\n        synonyms: [\"past month\"]\n        description: Filter to the last 30 calendar days.\n        expr: \"DATE >= DATEADD('day', -30, CURRENT_DATE())\"\n\n      - name: last_year\n        synonyms: [\"past year\", \"last 12 months\"]\n        description: Filter to the last 365 days.\n        expr: \"DATE >= DATEADD('day', -365, CURRENT_DATE())\"\n\nverified_queries:\n  - name: average_close_by_month\n    question: What was the average closing price by month for NVDA this year?\n    sql: >\n      SELECT DATE_TRUNC('month', DATE) AS month,\n             AVG(CLOSE_PRICE) AS avg_close\n      FROM SNOWFLAKE_INTELIGENCE_HOL.DATA.STOCK_PRICE_HISTORY\n      WHERE TICKER = 'NVDA'\n        AND DATE >= DATE_TRUNC('year', CURRENT_DATE())\n      GROUP BY 1\n      ORDER BY 1\n\n  - name: total_volume_last_30_days\n    question: How many shares traded for AAPL in the last 30 days?\n    sql: >\n      SELECT SUM(VOLUME) AS total_volume\n      FROM SNOWFLAKE_INTELIGENCE_HOL.DATA.STOCK_PRICE_HISTORY\n      WHERE TICKER = 'AAPL'\n        AND DATE >= DATEADD('day', -30, CURRENT_DATE())\n  $$\n  -- Omit third parameter or set to FALSE to create the view\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6b7b3ae-2b22-4c9b-be7e-ff65f280f43f",
   "metadata": {
    "language": "sql",
    "name": "TEST_SEARCH_SQL"
   },
   "outputs": [],
   "source": "SELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'SNOWFLAKE_DOCUMENTATION.SHARED.CKE_SNOWFLAKE_DOCS_SERVICE',\n      '{\n        \"query\": \"how do i configure a default schema for a snowflake user account\",\n        \"columns\":[\n            \"chunk\",\n            \"document_title\",\n            \"source_url\"\n        ],\n        \"limit\":10\n      }'\n  )\n)['results'] as results\n;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3dcb3d2-265a-4758-8d36-7cbbd7b5994b",
   "metadata": {
    "language": "python",
    "name": "PARSE_SQL_SEARCH_RESULT",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": " \nresult = cells.TEST_SEARCH_SQL.to_pandas()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb2d8192-e2d1-4294-a8cd-03d755355eae",
   "metadata": {
    "language": "python",
    "name": "RAW_CELL_OUTPUT",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "result",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a11920-3fe9-4ebc-9dae-86820b84a37e",
   "metadata": {
    "language": "python",
    "name": "CONVERT_SEARCH_RESULT_TO_DATAFRAME",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = pd.DataFrame(json.loads(result[\"RESULTS\"][0]))\ndf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6d0a745-b0cc-4817-b850-cb82cf9b936e",
   "metadata": {
    "name": "CORTEX_REST_API_DOCS_LINK",
    "collapsed": false
   },
   "source": "[Query Cortex Search REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/query-cortex-search-service#rest-api)"
  },
  {
   "cell_type": "code",
   "id": "8e22e87b-1561-4e5a-a7e8-5b3f13373e27",
   "metadata": {
    "language": "python",
    "name": "TEST_SEARCH_PYTHON",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4977c199-9949-4182-a87a-08fce4a3d636",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "## Configure Cortex Agent\n\nNow that we have all the pieces in place for the agent to pull context from, it's time to create the agent. The rest of the process will be done in Snowsight. Follow the steps below to complete the agent creation.\n\n\n### Step-by-Step Guide\n1. Create the Agent \n    - Go to AI & ML > Agents\n    - Select \"Create Agent\"\n    - Enter agent name and description\n2. Add Cortex Search Service\n    - Select \"Tools\" > \"Cortex Search Services\" > \"+ Add\"\n    - Select your existing Cortex Search service\n    - Enter chunk_id as ID column and document_name as title column.\n    - Enter name and description for the search service\n    - Click \"Add\"\n3. Add Cortex Analyst Tool (Using Semantic Model)\n    - Select \"Tools\" > \"Cortex Analyst\" > \"+ Add\"\n    - Enter name for the semantic model tool\n    - Select the semantic view\n    - Choose warehouse for query execution\n    - Set query timeout (seconds)\n    - Add description\n    - Click \"Add\"\n4. Add Custom Tools\n    - Select \"Tools\" > \"Custom Tools\" > \"+ Add\"\n    - Leave resource type as procedure\n    - Select existing stored procedure\n    - Configure parameters (name, type, description, required status)\n    - Select warehouse for execution (leave as default)\n    - Add tool description and optionally argument descriptions\n    - Click \"Add\"\n5. Add Custom Prompt/Instructions\n    - In the agent configuration, add custom instructions for:\n        * Response behavior\n        * Orchestration logic\n        * System prompts\n6. Save and Test\n    - Click \"Save\" to create the agent\n    - Test with sample questions\n    - Monitor agent performance and user feedback"
  },
  {
   "cell_type": "markdown",
   "id": "1a1c2b69-2773-4a12-9f1d-57028a6fff73",
   "metadata": {
    "name": "cell7"
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "73b097e2-f2d2-4db1-b357-ce0abea03269",
   "metadata": {
    "name": "cell9"
   },
   "source": ""
  }
 ]
}